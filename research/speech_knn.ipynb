{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# EEL4511 Real-time DSP Applications Lab 9 Final Project\n",
    "\n",
    "### Samuel Cuervo\n",
    "\n",
    "### Git Repo: https://github.com/scuervo101/TI-F28379D-SPEECH-RECOGNITION\n",
    "\n",
    "## Abstract:\n",
    "Using the codec and the DSP, I will be sampling voice triggered audio and attempting to recognize what word is spoken from a set of pre-trained words. The DSP will communicate back to the user using UART and the serial console on the computer. The voice recognition engine uses MFCC, a Hamming window filter on which FFT and mel spectrum transform is applied to extract a smaller set of descriptive features, and uses a KNN model as a classification tool for determining the label (which word) the audio sample belongs to."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# TI-F28379D-SPEECH-RECOGNITION\n",
    "\n",
    "## DSP Speech Recognition\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Planning on using KNN for the classification and MFCC feature extraction for the audio files\n",
    "- MFCC uses FFT feature extraction\n",
    "- Extracting from wav files and a test set provided by me using the microphone on the codec\n",
    "- Not sure how many test files will be needed, planning on doing 20-30 per word\n",
    "- Planning on using the TensorFlow Command Speech data set for training\n",
    "- Another possiblity is to run an FFT on the data and test its pitch (yes high pitch, no low pitch)\n",
    "- Python will handle the Pre-computation of the ML model's parameters\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The dsp will be continuously sampling the microphone. When audio is detected on the microphone (threshold is surpassed), it will begin to record the sampled data into the external SRAM. After recording the data into the SRAM, it will process the data in the background (ML processing, MFCC to KNN, or FFT). Responses from the DSP will be displayed through UART and a serial console.\n",
    "\n",
    "Plans for the DSP to have a simple conversation with the user (with responses that illustrate the response ie. If answer no, DSP respond with \"why not\")\n",
    "\n",
    "*Yes* and *No* will be the words initially featured. If the DSP can handle more, will add *Hello* and *Goodbye*\n",
    "\n",
    "Conversation theme is about the DSP taking over the world. Answer improperly and flash the RED LEDs or answer correctly and flash the BLUE LEDs\n",
    "\n",
    "Using the 8 bit LEDs for debugging the state.\n",
    "\n",
    "**Logical states:** \n",
    "\n",
    "- Idle (listening and waiting for a signal or a response on the microphone)\n",
    "- Recording (Storing the sampling data into the SRAM)\n",
    "- Processing (Running ML processing or FFT(During this point either save the next signal response or don't sample))\n",
    "\n",
    "After the processing is complete, trigger the DSPs response and send it through UART"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Python Speech Recognition Proof of Concept\n",
    "\n",
    "Below is a test implementation of the DSP speech recognition using MFCC for feature extraction and KNN for classification\n",
    "\n",
    "Testing the classification between yes and no\n",
    "\n",
    "In another notebook, I will do the proper K fold testing for finding the best possible parameters "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io  import wavfile\n",
    "from librosa.feature import mfcc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "source": [
    "## Data Set\n",
    "\n",
    "I created a personal data set of the words yes and no (24 samples for each word) for the proof of concept. For the final training data, I will use the larger [TensorFlow Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) data set.\n",
    "\n",
    "Below I import the data set\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading data:yes_0.wav\nReading data:no_0.wav\nReading data:yes_1.wav\nReading data:no_1.wav\nReading data:yes_2.wav\nReading data:no_2.wav\nReading data:yes_3.wav\nReading data:no_3.wav\nReading data:yes_4.wav\nReading data:no_4.wav\nReading data:yes_5.wav\nReading data:no_5.wav\nReading data:yes_6.wav\nReading data:no_6.wav\nReading data:yes_7.wav\nReading data:no_7.wav\nReading data:yes_8.wav\nReading data:no_8.wav\nReading data:yes_9.wav\nReading data:no_9.wav\nReading data:yes_10.wav\nReading data:no_10.wav\nReading data:yes_11.wav\nReading data:no_11.wav\nReading data:yes_12.wav\nReading data:no_12.wav\nReading data:yes_13.wav\nReading data:no_13.wav\nReading data:yes_14.wav\nReading data:no_14.wav\nReading data:yes_15.wav\nReading data:no_15.wav\nReading data:yes_16.wav\nReading data:no_16.wav\nReading data:yes_17.wav\nReading data:no_17.wav\nReading data:yes_18.wav\nReading data:no_18.wav\nReading data:yes_19.wav\nReading data:no_19.wav\nReading data:yes_20.wav\nReading data:no_20.wav\nReading data:yes_21.wav\nReading data:no_21.wav\nReading data:yes_22.wav\nReading data:no_22.wav\nReading data:yes_23.wav\nReading data:no_23.wav\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Read the sample data from DSP-DataSet\n",
    "for i in range(24):\n",
    "    print(\"Reading data:\" + \"yes_\" + str(i) + \".wav\")\n",
    "    samplerate, wav = wavfile.read(\"../../DSP-DataSet/yes_ds/yes_\" + str(i) + \".wav\")\n",
    "    data += [wav]\n",
    "    labels.append(1)\n",
    "\n",
    "    print(\"Reading data:\" + \"no_\" + str(i) + \".wav\")\n",
    "    samplerate, wav = wavfile.read(\"../../DSP-DataSet/no_ds/no_\" + str(i) + \".wav\")\n",
    "    data += [wav]\n",
    "    labels.append(0)\n",
    "\n",
    "data = np.array(data,dtype=\"float\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "source": [
    "## MFCC Feature Extraction\n",
    "\n",
    "In an attempt to reduce the model complexity, MFCC will extract the top 12 coefficients that describe the audio. This reduces the size of samples from 44100 elements (1 sec audio clips) to 12 elements.\n",
    "\n",
    "The MFCC function used here is using the Librosa library. I will be creating my own MFCC function (in C) using the DSP's FFT function and by manually applying the filtering."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48, 13)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "n_fft=512\n",
    "n_mfcc = 13\n",
    "samplerate, _ = wavfile.read(\"../../DSP-DataSet/yes_ds/yes_0.wav\")\n",
    "\n",
    "features = []\n",
    "for i in range(data.shape[0]):\n",
    "    features += [mfcc(y=data[i], sr=samplerate, n_mfcc=n_mfcc)]\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "processed_features = np.zeros((features.shape[0], features.shape[1]))\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(features.shape[1]):\n",
    "        mean = sum(features[i][j]) / len(features[i][j])\n",
    "        processed_features[i][j] = mean\n",
    "\n",
    "features = processed_features\n",
    "processed_features.shape"
   ]
  },
  {
   "source": [
    "## KNN Parameter Training\n",
    "\n",
    "I have to split the data set to train it with a separate group from testing. Before I can train the KNN model, I most run some form of validation to find the best parameters. I am using a sklearn function called GridSearchCV which will test parameters until it finds the best scoring set of parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features,labels,test_size=0.25)\n",
    "\n",
    "# Testing parameters to find the best set\n",
    "params = {\n",
    "    \"n_neighbors\" : list(range(3, 21, 2)),\n",
    "    \"weights\" : [\"uniform\", \"distance\"],\n",
    "    \"metric\" : [\"euclidean\", \"manhattan\", \"chebyshev\"]\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(KNeighborsClassifier(), params, verbose=1, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "results = GSCV.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The best parameters for KNN classification are: \n{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n\nWith a score of: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters for KNN classification are: \")\n",
    "print(results.best_params_)\n",
    "print(\"\\nWith a score of: \" + str(results.best_score_))"
   ]
  },
  {
   "source": [
    "### KNN Model Training\n",
    "\n",
    "After finding the best parameters, I will train the KNN model with the training data set and test it with the test data set. Once the model has been trained, I can extract the models weights to implement it in C for the DSP\n",
    "\n",
    "**Note** Due to the small sample size, the model reaches perfect accuracy which could mean it is overfitting or could be inaccurate in a real world scenerio. I will be expanding the data set later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted labels for the test set: \n[0 1 1 0 0 0 1 0 0 1 0 0]\nExpected labels for the test set: \n[0 1 1 0 0 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\", weights=\"uniform\")\n",
    "knn = knn.fit(features_train,labels_train)\n",
    "\n",
    "pred = knn.predict(features_test)\n",
    "\n",
    "print(\"Predicted labels for the test set: \")\n",
    "print(pred)\n",
    "print(\"Expected labels for the test set: \")\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}